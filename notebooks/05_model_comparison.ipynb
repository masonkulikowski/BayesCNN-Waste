{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: Bayes Classifier vs Custom CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch_directml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "from src.config import load_config\n",
    "from src.load_data import load_data, TrashNetDataset\n",
    "from src.transforms import get_transforms\n",
    "from src.models.bayes import BayesClassifier\n",
    "from src.models.cnn import create_custom_cnn\n",
    "from src.evaluation import load_model_checkpoint, evaluate_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "config = load_config()\n",
    "device = torch_directml.device()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Dataset: {config['data']['dataset_name']}\")\n",
    "print(f\"Classes: {config['data']['classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_hf = load_data(split_data=True)\n",
    "print(f\"Test samples: {len(test_hf)}\")\n",
    "\n",
    "val_transform = get_transforms(config, split='val')\n",
    "test_dataset = TrashNetDataset(test_hf, transform=val_transform)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_path = Path(config['paths']['models_dir']) / 'bayes_classifier.pkl'\n",
    "bayes_classifier = BayesClassifier.load(bayes_path)\n",
    "print(f\"Loaded Bayes from: {bayes_path}\")\n",
    "\n",
    "cnn_model = create_custom_cnn(num_classes=config['data']['num_classes'])\n",
    "cnn_path = Path(config['paths']['models_dir']) / 'custom_cnn_best.pth'\n",
    "cnn_model = load_model_checkpoint(cnn_model, cnn_path, device=device)\n",
    "cnn_model.eval()\n",
    "print(f\"Loaded CNN from: {cnn_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_start = time.time()\n",
    "bayes_results = bayes_classifier.evaluate(test_hf, verbose=True)\n",
    "bayes_time = time.time() - bayes_start\n",
    "\n",
    "bayes_preds = bayes_results['predictions']\n",
    "bayes_labels = bayes_results['true_labels']\n",
    "bayes_acc = bayes_results['accuracy']\n",
    "bayes_cm = bayes_results['confusion_matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_start = time.time()\n",
    "cnn_acc, cnn_preds, cnn_labels, cnn_cm = evaluate_model(\n",
    "    cnn_model, test_loader, device, model_name='Custom CNN'\n",
    ")\n",
    "cnn_time = time.time() - cnn_start\n",
    "\n",
    "print(f\"CNN Eval Time: {cnn_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(cnn_labels)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy (%)', 'Eval Time (s)', 'Throughput (samples/s)', 'Time per Image (ms)'],\n",
    "    'Bayes': [\n",
    "        f\"{bayes_acc*100:.2f}\",\n",
    "        f\"{bayes_time:.2f}\",\n",
    "        f\"{len(test_hf)/bayes_time:.2f}\",\n",
    "        f\"{bayes_time/len(test_hf)*1000:.2f}\"\n",
    "    ],\n",
    "    'CNN': [\n",
    "        f\"{cnn_acc:.2f}\",\n",
    "        f\"{cnn_time:.2f}\",\n",
    "        f\"{total/cnn_time:.2f}\",\n",
    "        f\"{cnn_time/total*1000:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = config['data']['classes']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "bayes_class_acc = [100 * bayes_cm[i, i] / bayes_cm[i].sum() for i in range(num_classes)]\n",
    "cnn_class_acc = [100 * cnn_cm[i, i] / cnn_cm[i].sum() for i in range(num_classes)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(num_classes)\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, bayes_class_acc, width, label='Bayes', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + width/2, cnn_class_acc, width, label='CNN', color='coral', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_title('Per-Class Accuracy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "for i, (b, c) in enumerate(zip(bayes_class_acc, cnn_class_acc)):\n",
    "    ax.text(i - width/2, b + 1, f'{b:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + width/2, c + 1, f'{c:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(config['paths']['results_dir']) / 'per_class_accuracy.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY (%)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Class':<12} {'Bayes':>10} {'CNN':>10} {'Diff':>10}\")\n",
    "print(\"-\"*60)\n",
    "for i, name in enumerate(class_names):\n",
    "    diff = cnn_class_acc[i] - bayes_class_acc[i]\n",
    "    print(f\"{name:<12} {bayes_class_acc[i]:>10.2f} {cnn_class_acc[i]:>10.2f} {diff:>+10.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(bayes_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "axes[0].set_title(f'Bayes Classifier')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "sns.heatmap(cnn_cm, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "axes[1].set_title(f'Custom CNN')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(config['paths']['results_dir']) / 'confusion_matrices.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAYES CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(bayes_labels, bayes_preds, target_names=class_names, digits=4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CUSTOM CNN\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(cnn_labels, cnn_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_correct = (bayes_preds == bayes_labels) & (cnn_preds == cnn_labels)\n",
    "both_wrong = (bayes_preds != bayes_labels) & (cnn_preds != cnn_labels)\n",
    "bayes_only = (bayes_preds == bayes_labels) & (cnn_preds != cnn_labels)\n",
    "cnn_only = (bayes_preds != bayes_labels) & (cnn_preds == cnn_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL AGREEMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Both correct:    {both_correct.sum():>4} ({100*both_correct.sum()/len(test_hf):>5.1f}%)\")\n",
    "print(f\"Both wrong:      {both_wrong.sum():>4} ({100*both_wrong.sum()/len(test_hf):>5.1f}%)\")\n",
    "print(f\"Only Bayes:      {bayes_only.sum():>4} ({100*bayes_only.sum()/len(test_hf):>5.1f}%)\")\n",
    "print(f\"Only CNN:        {cnn_only.sum():>4} ({100*cnn_only.sum()/len(test_hf):>5.1f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "data = [both_correct.sum(), cnn_only.sum(), bayes_only.sum(), both_wrong.sum()]\n",
    "labels = ['Both Correct', 'CNN Only', 'Bayes Only', 'Both Wrong']\n",
    "colors = ['#2ecc71', '#e67e22', '#3498db', '#e74c3c']\n",
    "\n",
    "ax.pie(data, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title('Model Agreement')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(config['paths']['results_dir']) / 'agreement.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_size_kb = bayes_path.stat().st_size / 1024\n",
    "cnn_size_mb = cnn_path.stat().st_size / (1024 * 1024)\n",
    "cnn_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPLEXITY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'Bayes':<15} {'CNN':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Size':<25} {f'{bayes_size_kb:.1f} KB':<15} {f'{cnn_size_mb:.1f} MB':<15}\")\n",
    "print(f\"{'Parameters':<25} {'~1,000':<15} {f'{cnn_params:,}':<15}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    'bayes': {\n",
    "        'accuracy': float(bayes_acc),\n",
    "        'eval_time_s': float(bayes_time),\n",
    "        'throughput': float(len(test_hf)/bayes_time),\n",
    "        'size_kb': float(bayes_size_kb),\n",
    "        'per_class': {class_names[i]: float(bayes_class_acc[i]) for i in range(num_classes)},\n",
    "        'confusion_matrix': bayes_cm.tolist()\n",
    "    },\n",
    "    'cnn': {\n",
    "        'accuracy': float(cnn_acc),\n",
    "        'eval_time_s': float(cnn_time),\n",
    "        'throughput': float(total/cnn_time),\n",
    "        'size_mb': float(cnn_size_mb),\n",
    "        'parameters': int(cnn_params),\n",
    "        'per_class': {class_names[i]: float(cnn_class_acc[i]) for i in range(num_classes)},\n",
    "        'confusion_matrix': cnn_cm.tolist()\n",
    "    },\n",
    "    'agreement': {\n",
    "        'both_correct': int(both_correct.sum()),\n",
    "        'both_wrong': int(both_wrong.sum()),\n",
    "        'bayes_only': int(bayes_only.sum()),\n",
    "        'cnn_only': int(cnn_only.sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "save_path = Path(config['paths']['results_dir']) / 'comparison_results.json'\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayescnn-directml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
