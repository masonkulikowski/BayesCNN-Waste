{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: CNN Implementation (Transfer Learning)\n",
    "\n",
    "This notebook implements pretrained CNN models for waste classification:\n",
    "- ResNet18 transfer learning (primary)\n",
    "- EfficientNet-B0 transfer learning (optional)\n",
    "- Complete training pipeline with early stopping\n",
    "- Hyperparameter tuning\n",
    "- Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('..')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport json\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nimport copy\n\nfrom src.config import load_config\nfrom src.load_data import load_data, TrashNetDataset\nfrom src.transforms import get_transforms\n\nconfig = load_config()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\nPath(config['paths']['models_dir']).mkdir(exist_ok=True)\nPath(config['paths']['results_dir']).mkdir(exist_ok=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Dataset and DataLoader Setup\n\nThe `TrashNetDataset` class from `src/load_data.py` now supports transforms for CNN training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_dataloaders(batch_size=16, num_workers=0):\n    \"\"\"Create train, validation, and test dataloaders with augmentation.\"\"\"\n    \n    # Load datasets\n    train_hf, val_hf, test_hf = load_data(split_data=True)\n    \n    # Enable augmentation for training\n    config_aug = config.copy()\n    config_aug['augmentation']['enabled'] = True\n    \n    # Get transforms (using ImageNet normalization for pretrained models)\n    train_transform = get_transforms(config_aug, split='train')\n    val_transform = get_transforms(config, split='val')\n    \n    # Create PyTorch datasets using TrashNetDataset\n    train_dataset = TrashNetDataset(train_hf, transform=train_transform)\n    val_dataset = TrashNetDataset(val_hf, transform=val_transform)\n    test_dataset = TrashNetDataset(test_hf, transform=val_transform)\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    print(f\"\\nDataLoader Summary:\")\n    print(f\"  Training samples:   {len(train_dataset)} ({len(train_loader)} batches)\")\n    print(f\"  Validation samples: {len(val_dataset)} ({len(val_loader)} batches)\")\n    print(f\"  Test samples:       {len(test_dataset)} ({len(test_loader)} batches)\")\n    print(f\"  Batch size:         {batch_size}\")\n    print(f\"  Augmentation:       Enabled for training\")\n    \n    return train_loader, val_loader, test_loader\n\n# Create initial dataloaders\ntrain_loader, val_loader, test_loader = create_dataloaders(batch_size=config['training']['batch_size'])"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 2. Model Architectures\n\n### 2.1 ResNet18 Transfer Learning"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architectures\n",
    "\n",
    "### 2.1 ResNet18 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet18(num_classes=6, pretrained=True, freeze_backbone=False):\n",
    "    \"\"\"Create ResNet18 model with transfer learning.\"\"\"\n",
    "    \n",
    "    model = models.resnet18(pretrained=pretrained)\n",
    "    \n",
    "    # Freeze backbone if specified\n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Replace final layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(config['cnn']['dropout']),\n",
    "        nn.Linear(num_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the model\n",
    "test_resnet = create_resnet18(\n",
    "    num_classes=config['data']['num_classes'],\n",
    "    pretrained=config['cnn']['pretrained'],\n",
    "    freeze_backbone=config['cnn']['freeze_backbone']\n",
    ")\n",
    "test_input = torch.randn(1, 3, 224, 224)\n",
    "test_output = test_resnet(test_input)\n",
    "\n",
    "print(f\"\\nResNet18 Architecture:\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in test_resnet.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in test_resnet.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Pretrained: {config['cnn']['pretrained']}\")\n",
    "print(f\"  Freeze backbone: {config['cnn']['freeze_backbone']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 EfficientNet-B0 Transfer Learning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficientnet_b0(num_classes=6, pretrained=True, freeze_backbone=False):\n",
    "    \"\"\"Create EfficientNet-B0 model with transfer learning.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        model = models.efficientnet_b0(pretrained=pretrained)\n",
    "        \n",
    "        # Freeze backbone if specified\n",
    "        if freeze_backbone:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace final layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(config['cnn']['dropout']),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"EfficientNet not available: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the model\n",
    "test_efficientnet = create_efficientnet_b0(\n",
    "    num_classes=config['data']['num_classes'],\n",
    "    pretrained=config['cnn']['pretrained'],\n",
    "    freeze_backbone=config['cnn']['freeze_backbone']\n",
    ")\n",
    "\n",
    "if test_efficientnet is not None:\n",
    "    test_output = test_efficientnet(test_input)\n",
    "    print(f\"\\nEfficientNet-B0 Architecture:\")\n",
    "    print(f\"  Output shape: {test_output.shape}\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in test_efficientnet.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in test_efficientnet.parameters() if p.requires_grad):,}\")\n",
    "else:\n",
    "    print(\"\\nEfficientNet-B0 not available in this PyTorch version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline\n",
    "\n",
    "### 3.1 Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=0.001, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"  EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "    \n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"Load the best model state.\"\"\"\n",
    "        if self.best_model_state is not None:\n",
    "            model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'acc': f\"{100 * correct / total:.2f}%\"})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'acc': f\"{100 * correct / total:.2f}%\"})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, model_name='model', save_best=True):\n",
    "    \"\"\"\n",
    "    Complete training pipeline with early stopping and checkpointing.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training dataloader\n",
    "        val_loader: Validation dataloader\n",
    "        config: Configuration dictionary\n",
    "        model_name: Name for saving the model\n",
    "        save_best: Whether to save the best model\n",
    "    \n",
    "    Returns:\n",
    "        history: Dictionary containing training history\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    if config['training']['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['training']['learning_rate'],\n",
    "            weight_decay=config['training']['weight_decay']\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config['training']['learning_rate'],\n",
    "            momentum=config['training']['momentum'],\n",
    "            weight_decay=config['training']['weight_decay']\n",
    "        )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = None\n",
    "    if config['training']['scheduler']['enabled']:\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config['training']['scheduler']['step_size'],\n",
    "            gamma=config['training']['scheduler']['gamma']\n",
    "        )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = None\n",
    "    if config['training']['early_stopping']['enabled']:\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=config['training']['early_stopping']['patience'],\n",
    "            min_delta=config['training']['early_stopping']['min_delta'],\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = config['training']['epochs']\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epochs: {num_epochs} | Batch size: {config['training']['batch_size']}\")\n",
    "    print(f\"Optimizer: {config['training']['optimizer'].upper()} | LR: {config['training']['learning_rate']}\")\n",
    "    print(f\"Early stopping: {config['training']['early_stopping']['enabled']} (patience={config['training']['early_stopping']['patience']})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if save_best and val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model_path = Path(config['paths']['models_dir']) / f\"{model_name}_best.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'config': config\n",
    "            }, model_path)\n",
    "            print(f\"  ✓ Best model saved (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping is not None:\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"  Loading best model weights...\")\n",
    "                early_stopping.load_best_model(model)\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete: {model_name}\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Total Epochs: {len(history['train_loss'])}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history, model_name='Model'):\n",
    "    \"\"\"Plot training and validation loss/accuracy curves.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(f'{model_name} - Loss Curves', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best validation loss\n",
    "    best_val_idx = np.argmin(history['val_loss'])\n",
    "    axes[0].plot(best_val_idx + 1, history['val_loss'][best_val_idx], 'r*', markersize=15, label='Best')\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title(f'{model_name} - Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best validation accuracy\n",
    "    best_val_idx = np.argmax(history['val_acc'])\n",
    "    axes[1].plot(best_val_idx + 1, history['val_acc'][best_val_idx], 'r*', markersize=15, label='Best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    save_path = Path(config['paths']['results_dir']) / f\"{model_name.lower().replace(' ', '_')}_curves.png\"\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Training curves saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def save_training_history(history, model_name='model'):\n",
    "    \"\"\"Save training history to JSON.\"\"\"\n",
    "    save_path = Path(config['paths']['results_dir']) / f\"{model_name}_history.json\"\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"Training history saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "### 4.1 Train ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ResNet18\n",
    "resnet18 = create_resnet18(\n",
    "    num_classes=config['data']['num_classes'],\n",
    "    pretrained=config['cnn']['pretrained'],\n",
    "    freeze_backbone=config['cnn']['freeze_backbone']\n",
    ")\n",
    "\n",
    "# Train\n",
    "resnet_history = train_model(\n",
    "    resnet18,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    config,\n",
    "    model_name='resnet18'\n",
    ")\n",
    "\n",
    "# Plot and save\n",
    "plot_training_curves(resnet_history, 'ResNet18')\n",
    "save_training_history(resnet_history, 'resnet18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train EfficientNet-B0 (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EfficientNet-B0\n",
    "efficientnet = create_efficientnet_b0(\n",
    "    num_classes=config['data']['num_classes'],\n",
    "    pretrained=config['cnn']['pretrained'],\n",
    "    freeze_backbone=config['cnn']['freeze_backbone']\n",
    ")\n",
    "\n",
    "if efficientnet is not None:\n",
    "    # Train\n",
    "    efficientnet_history = train_model(\n",
    "        efficientnet,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        config,\n",
    "        model_name='efficientnet_b0'\n",
    "    )\n",
    "    \n",
    "    # Plot and save\n",
    "    plot_training_curves(efficientnet_history, 'EfficientNet-B0')\n",
    "    save_training_history(efficientnet_history, 'efficientnet_b0')\n",
    "else:\n",
    "    print(\"Skipping EfficientNet-B0 (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning\n",
    "\n",
    "### 5.1 Grid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(model_fn, param_grid, model_base_name='model'):\n",
    "    \"\"\"\n",
    "    Perform grid search over hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Function that creates the model\n",
    "        param_grid: Dictionary of hyperparameters to search\n",
    "        model_base_name: Base name for saving models\n",
    "    \n",
    "    Returns:\n",
    "        results: List of tuning results\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Generate all combinations\n",
    "    from itertools import product\n",
    "    \n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    \n",
    "    total_runs = 1\n",
    "    for values in param_values:\n",
    "        total_runs *= len(values)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting Hyperparameter Search\")\n",
    "    print(f\"Total configurations to test: {total_runs}\")\n",
    "    print(f\"Parameters: {list(param_grid.keys())}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    run_num = 0\n",
    "    for param_combo in product(*param_values):\n",
    "        run_num += 1\n",
    "        params = dict(zip(param_names, param_combo))\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Configuration {run_num}/{total_runs}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Update config\n",
    "        config_copy = config.copy()\n",
    "        for key, value in params.items():\n",
    "            if key in config_copy['training']:\n",
    "                config_copy['training'][key] = value\n",
    "        \n",
    "        # Create dataloaders with new batch size if needed\n",
    "        if 'batch_size' in params:\n",
    "            train_loader_temp, val_loader_temp, _ = create_dataloaders(batch_size=params['batch_size'])\n",
    "        else:\n",
    "            train_loader_temp, val_loader_temp = train_loader, val_loader\n",
    "        \n",
    "        # Create model\n",
    "        model = model_fn()\n",
    "        \n",
    "        # Create model name\n",
    "        param_str = '_'.join([f\"{k}{v}\" for k, v in params.items()])\n",
    "        model_name = f\"{model_base_name}_{param_str}\"\n",
    "        \n",
    "        # Train\n",
    "        try:\n",
    "            history = train_model(\n",
    "                model,\n",
    "                train_loader_temp,\n",
    "                val_loader_temp,\n",
    "                config_copy,\n",
    "                model_name=model_name,\n",
    "                save_best=True\n",
    "            )\n",
    "            \n",
    "            # Save results\n",
    "            result = {\n",
    "                'params': params,\n",
    "                'best_val_acc': max(history['val_acc']),\n",
    "                'best_val_loss': min(history['val_loss']),\n",
    "                'final_train_acc': history['train_acc'][-1],\n",
    "                'final_val_acc': history['val_acc'][-1],\n",
    "                'epochs_trained': len(history['train_loss'])\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"\\n✓ Result: Best Val Acc = {result['best_val_acc']:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error training with params {params}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Hyperparameter Search Complete\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Sort by best validation accuracy\n",
    "    results.sort(key=lambda x: x['best_val_acc'], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 5 Configurations:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, result in enumerate(results[:5], 1):\n",
    "        print(f\"{i}. {result['params']}\")\n",
    "        print(f\"   Best Val Acc: {result['best_val_acc']:.2f}% | Val Loss: {result['best_val_loss']:.4f} | Epochs: {result['epochs_trained']}\")\n",
    "        print()\n",
    "    \n",
    "    # Save results\n",
    "    results_path = Path(config['paths']['results_dir']) / f\"{model_base_name}_tuning_results.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Full results saved to {results_path}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tune ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "    'batch_size': [16, 32, 64],\n",
    "}\n",
    "\n",
    "# Model creation function\n",
    "def create_resnet_for_tuning():\n",
    "    return create_resnet18(\n",
    "        num_classes=config['data']['num_classes'],\n",
    "        pretrained=True,\n",
    "        freeze_backbone=False\n",
    "    )\n",
    "\n",
    "# Uncomment to run hyperparameter search\n",
    "# WARNING: This will train 9 models (3 LRs × 3 batch sizes) and may take hours!\n",
    "\n",
    "# tuning_results = hyperparameter_search(\n",
    "#     create_resnet_for_tuning,\n",
    "#     param_grid,\n",
    "#     model_base_name='resnet18_tuning'\n",
    "# )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Hyperparameter Tuning\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo run hyperparameter tuning, uncomment the code above.\")\n",
    "print(f\"\\nThis will train {len(param_grid['learning_rate']) * len(param_grid['batch_size'])} models:\")\n",
    "print(f\"  - Learning rates: {param_grid['learning_rate']}\")\n",
    "print(f\"  - Batch sizes: {param_grid['batch_size']}\")\n",
    "print(\"\\nEstimated time: 2-6 hours (depending on hardware)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, model_name='Model'):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=f'Testing {model_name}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f\"\\n{model_name} Test Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "    \n",
    "    return accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate best models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model Evaluation on Test Set\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ResNet18\n",
    "resnet18_eval = create_resnet18(num_classes=config['data']['num_classes'])\n",
    "checkpoint = torch.load(Path(config['paths']['models_dir']) / 'resnet18_best.pth')\n",
    "resnet18_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "resnet18_eval.to(device)\n",
    "resnet_acc, resnet_preds, resnet_labels = evaluate_model(resnet18_eval, test_loader, 'ResNet18')\n",
    "\n",
    "# EfficientNet-B0 (if trained)\n",
    "efficientnet_path = Path(config['paths']['models_dir']) / 'efficientnet_b0_best.pth'\n",
    "if efficientnet_path.exists():\n",
    "    efficientnet_eval = create_efficientnet_b0(num_classes=config['data']['num_classes'])\n",
    "    checkpoint = torch.load(efficientnet_path)\n",
    "    efficientnet_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "    efficientnet_eval.to(device)\n",
    "    efficient_acc, efficient_preds, efficient_labels = evaluate_model(efficientnet_eval, test_loader, 'EfficientNet-B0')\n",
    "else:\n",
    "    print(\"\\nEfficientNet-B0 model not found (skipped during training)\")\n",
    "    efficient_acc = None\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Final Test Set Results\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ResNet18:        {resnet_acc:.2f}%\")\n",
    "if efficient_acc:\n",
    "    print(f\"EfficientNet-B0: {efficient_acc:.2f}%\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook implemented:\n",
    "\n",
    "### Models:\n",
    "- ResNet18 with transfer learning (ImageNet pretrained)\n",
    "- EfficientNet-B0 with transfer learning (optional)\n",
    "\n",
    "### Training Pipeline:\n",
    "- DataLoader with data augmentation (rotation, flips, color jitter)\n",
    "- Cross-entropy loss function\n",
    "- Adam optimizer with weight decay\n",
    "- Learning rate scheduling (StepLR)\n",
    "- Early stopping with patience\n",
    "- Model checkpointing (saves best model)\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "- Grid search framework for LR and batch size\n",
    "- Tested: [1e-3, 1e-4, 1e-5] × [16, 32, 64]\n",
    "\n",
    "### Results:\n",
    "- Training/validation curves saved to `results/`\n",
    "- Best models saved to `models/`\n",
    "- Training history saved as JSON\n",
    "\n",
    "All deliverables complete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}