{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Classifier Training\n",
    "\n",
    "Train and evaluate a Naive Bayes classifier with optimized feature engineering:\n",
    "\n",
    "### Base Features (52 total):\n",
    "- **Color**: Mean & Std HSV (6 features)\n",
    "- **Texture**: LBP histogram + Haralick (19 features)\n",
    "- **Shape**: Aspect ratio + Hu moments (4 features)\n",
    "\n",
    "### Material-Specific Features (7 total - optimized):\n",
    "- **Specular (Metal)**: Highlight contrast, gradient concentration (2)\n",
    "- **Metal**: Reflection directionality (1)\n",
    "- **Glass**: Brightness gradient smoothness, high-freq FFT energy, saturation uniformity (3)\n",
    "- **Trash**: Texture chaos (1)\n",
    "\n",
    "### Texture Features (16):\n",
    "- **Single-scale LBP**: Radius=2 only (16 bins)\n",
    "\n",
    "### Optimization Pipeline:\n",
    "1. **Correlation Removal**: 52 → ~43 features (threshold=0.85)\n",
    "2. **Scaling**: StandardScaler normalization\n",
    "3. **PCA**: ~43 → 15 components (optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import load_config\n",
    "from src.load_data import load_data\n",
    "from src.models.bayes import BayesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {config['data']['dataset_name']}\")\n",
    "print(f\"  Classes: {', '.join(config['data']['classes'])}\")\n",
    "print(f\"  Image size: {config['data']['image_size']}x{config['data']['image_size']}\")\n",
    "print(f\"\\nFeature extraction:\")\n",
    "print(f\"  LBP bins: {config['bayes']['lbp_bins']}\")\n",
    "print(f\"  Use balanced priors: {config['bayes']['use_balanced_priors']}\")\n",
    "print(f\"  Use OVR ensemble: {config['bayes']['use_ovr']}\")\n",
    "print(f\"\\nFeature optimization:\")\n",
    "print(f\"  Remove correlated: {config['bayes']['remove_correlated']}\")\n",
    "if config['bayes']['remove_correlated']:\n",
    "    print(f\"  Correlation threshold: {config['bayes']['correlation_threshold']}\")\n",
    "print(f\"  Use PCA: {config['bayes']['apply_pca']}\")\n",
    "if config['bayes']['apply_pca']:\n",
    "    print(f\"  PCA components: {config['bayes']['pca_components']}\")\n",
    "print(f\"\\n  Optimized feature breakdown:\")\n",
    "print(f\"    Color (HSV mean + std): 6\")\n",
    "print(f\"    Texture (LBP + Haralick): 19\")\n",
    "print(f\"    Shape (aspect + Hu): 4\")\n",
    "print(f\"    Specular (optimized): 2\")\n",
    "print(f\"    Metal (optimized): 1\")\n",
    "print(f\"    Glass (optimized): 3\")\n",
    "print(f\"    Trash (optimized): 1\")\n",
    "print(f\"    Single-scale LBP (r=2): 16\")\n",
    "print(\"    -----------------------------\")\n",
    "print(f\"    Base total: 52 features\")\n",
    "if config['bayes']['remove_correlated']:\n",
    "    print(f\"    After correlation removal: ~43 features\")\n",
    "if config['bayes']['apply_pca']:\n",
    "    print(f\"    After PCA: {config['bayes']['pca_components']} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "train_dataset, val_dataset, test_dataset = load_data(split_data=True)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training:   {len(train_dataset)} images\")\n",
    "print(f\"  Validation: {len(val_dataset)} images\")\n",
    "print(f\"  Test:       {len(test_dataset)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Visualization Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.bayes import BayesFeatureExtractor\n",
    "import cv2\n",
    "\n",
    "feature_extractor = BayesFeatureExtractor(config)\n",
    "\n",
    "sample = train_dataset[0]\n",
    "sample_image = sample['image']\n",
    "sample_label = sample['label']\n",
    "class_name = config['data']['classes'][sample_label]\n",
    "\n",
    "sample_image_resized = sample_image.resize((config['data']['image_size'], config['data']['image_size']))\n",
    "sample_array = np.array(sample_image_resized)\n",
    "\n",
    "color_feat = feature_extractor.extract_color_features(sample_image_resized)\n",
    "texture_feat = feature_extractor.extract_texture_features(sample_image_resized)\n",
    "shape_feat = feature_extractor.extract_shape_features(sample_image_resized)\n",
    "specular_feat = feature_extractor.extract_specular_features(sample_image_resized)\n",
    "metal_feat = feature_extractor.extract_metal_features(sample_image_resized)\n",
    "glass_feat = feature_extractor.extract_glass_features(sample_image_resized)\n",
    "trash_feat = feature_extractor.extract_trash_features(sample_image_resized)\n",
    "all_features = feature_extractor.extract_features(sample_image_resized)\n",
    "\n",
    "hsv = cv2.cvtColor(sample_array, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 4, hspace=0.35, wspace=0.3)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.imshow(sample_image_resized)\n",
    "ax1.set_title(f'Original Image\\nClass: {class_name}')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.imshow(hsv[:, :, 0], cmap='hsv')\n",
    "ax2.set_title('Hue Channel')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "gray = cv2.cvtColor(sample_array, cv2.COLOR_RGB2GRAY)\n",
    "grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "ax3.imshow(grad_magnitude, cmap='hot')\n",
    "ax3.set_title('Gradient Magnitude\\n(Metal Detection)')\n",
    "ax3.axis('off')\n",
    "\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "f_transform = np.fft.fft2(gray)\n",
    "f_shift = np.fft.fftshift(f_transform)\n",
    "magnitude = np.log(np.abs(f_shift) + 1)\n",
    "ax4.imshow(magnitude, cmap='viridis')\n",
    "ax4.set_title('FFT Spectrum\\n(Glass Detection)')\n",
    "ax4.axis('off')\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1, 0])\n",
    "ax5.bar(range(6), color_feat, color=['red', 'green', 'blue', 'red', 'green', 'blue'], alpha=0.7)\n",
    "ax5.set_xticks(range(6))\n",
    "ax5.set_xticklabels(['H_mu', 'S_mu', 'V_mu', 'H_sigma', 'S_sigma', 'V_sigma'], fontsize=7)\n",
    "ax5.set_title('Color (6)')\n",
    "ax5.set_ylabel('Value')\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1, 1])\n",
    "metal_combined = np.concatenate([specular_feat, metal_feat])\n",
    "ax6.bar(range(3), metal_combined, color='gold', alpha=0.7)\n",
    "ax6.set_xticks(range(3))\n",
    "ax6.set_xticklabels(['HighContrast', 'GradConc', 'RefDir'], fontsize=7)\n",
    "ax6.set_title('Metal Features (3)\\nOptimized from 8')\n",
    "ax6.set_ylabel('Value')\n",
    "ax6.grid(alpha=0.3)\n",
    "\n",
    "ax7 = fig.add_subplot(gs[1, 2])\n",
    "ax7.bar(range(3), glass_feat, color='lightblue', alpha=0.7)\n",
    "ax7.set_xticks(range(3))\n",
    "ax7.set_xticklabels(['BrGrad', 'HighFreq', 'SatUnif'], fontsize=7)\n",
    "ax7.set_title('Glass Features (3)\\nOptimized from 4')\n",
    "ax7.set_ylabel('Value')\n",
    "ax7.grid(alpha=0.3)\n",
    "\n",
    "ax8 = fig.add_subplot(gs[1, 3])\n",
    "ax8.bar([0], trash_feat, color='brown', alpha=0.7, width=0.5)\n",
    "ax8.set_xticks([0])\n",
    "ax8.set_xticklabels(['TexChaos'], fontsize=8)\n",
    "ax8.set_title('Trash Feature (1)\\nOptimized from 4')\n",
    "ax8.set_ylabel('Value')\n",
    "ax8.set_xlim(-0.5, 0.5)\n",
    "ax8.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Optimized Feature Extraction (52 features -> ~43 -> 15 via PCA)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OPTIMIZED Feature Breakdown:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Color (HSV): {len(color_feat)} features\")\n",
    "print(f\"  Texture (LBP + Haralick): {len(texture_feat)} features\")\n",
    "print(f\"  Shape (aspect + Hu): {len(shape_feat)} features\")\n",
    "print(f\"  Specular (optimized): {len(specular_feat)} features (was 6)\")\n",
    "print(f\"  Metal (optimized): {len(metal_feat)} features (was 2)\")\n",
    "print(f\"  Glass (optimized): {len(glass_feat)} features (was 4)\")\n",
    "print(f\"  Trash (optimized): {len(trash_feat)} features (was 4)\")\n",
    "print(\"  ------------------------------\")\n",
    "print(f\"  TOTAL EXTRACTED: {len(all_features)} features\")\n",
    "print(f\"  After PCA: {config['bayes']['pca_components']} features\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nKey optimized feature values for '{class_name}':\")\n",
    "print(f\"  Metal - Highlight contrast: {specular_feat[0]:.4f}\")\n",
    "print(f\"  Metal - Gradient concentration: {specular_feat[1]:.4f}\")\n",
    "print(f\"  Metal - Reflection directionality: {metal_feat[0]:.4f}\")\n",
    "print(f\"  Glass - Brightness gradient: {glass_feat[0]:.4f}\")\n",
    "print(f\"  Glass - High-freq energy: {glass_feat[1]:.4f}\")\n",
    "print(f\"  Glass - Saturation uniformity: {glass_feat[2]:.4f}\")\n",
    "print(f\"  Trash - Texture chaos: {trash_feat[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Bayes Classifier...\n",
      "  Balanced priors: True\n",
      "  Data augmentation: False\n",
      "Extracting features from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training features:   0%|          | 0/3537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training features: 100%|██████████| 3537/3537 [09:26<00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (3537, 52)\n",
      "Removing correlated features (threshold=0.85)...\n",
      "  Warning: Found 7 zero-variance features\n",
      "  Removing zero-variance features before correlation analysis...\n",
      "  Removed 16 correlated features\n",
      "  Features: 52 → 29 (removed 23 total)\n",
      "Standardizing features...\n",
      "Applying PCA (reducing to 15 components)...\n",
      "  Variance explained: 93.41%\n",
      "Training GaussianNB classifier...\n",
      "  Final feature shape: (3537, 15)\n",
      "Training complete!\n",
      "Training accuracy: 0.5824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.models.bayes.BayesClassifier at 0x196035dda10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_balanced = config['bayes']['use_balanced_priors']\n",
    "use_augmentation = config['augmentation']['enabled']\n",
    "augment_factor = 2\n",
    "\n",
    "print(f\"Initializing Bayes Classifier...\")\n",
    "print(f\"  Balanced priors: {use_balanced}\")\n",
    "print(f\"  Data augmentation: {use_augmentation}\")\n",
    "if use_augmentation:\n",
    "    print(f\"  Augmentation factor: {augment_factor}\")\n",
    "    print(f\"  This will triple the training data: {len(train_dataset)} -> {len(train_dataset) * (1 + augment_factor)}\\n\")\n",
    "\n",
    "classifier = BayesClassifier(config, use_balanced_priors=use_balanced)\n",
    "\n",
    "classifier.fit(train_dataset, verbose=True, use_augmentation=use_augmentation, augment_factor=augment_factor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "log_dir = Path(config[\"paths\"][\"logs_dir\"])\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_log_file = log_dir / \"validation_predictions.txt\"\n",
    "\n",
    "print(f\"\\nEvaluating on validation set...\")\n",
    "print(f\"Logging predictions to {val_log_file}\")\n",
    "val_results = classifier.evaluate(val_dataset, verbose=True, log_predictions=True, log_file=val_log_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_file = log_dir / \"test_predictions.txt\"\n",
    "\n",
    "print(f\"\\nEvaluating on test set...\")\n",
    "print(f\"Logging predictions to {test_log_file}\")\n",
    "test_results = classifier.evaluate(test_dataset, verbose=True, log_predictions=True, log_file=test_log_file)\n",
    "\n",
    "print(f\"\\n All prediction logs saved to {log_dir}\")\n",
    "print(f\"  - {val_log_file.name}\")\n",
    "print(f\"  - {test_log_file.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = test_results['confusion_matrix']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm_test,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Greens',\n",
    "    xticklabels=config['data']['classes'],\n",
    "    yticklabels=config['data']['classes']\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, class_name in enumerate(config['data']['classes']):\n",
    "    class_acc = cm_test[i, i] / cm_test[i].sum() if cm_test[i].sum() > 0 else 0\n",
    "    print(f\"  {class_name:<12}: {class_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Validation and Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "val_scores = [val_results[m] for m in metrics]\n",
    "test_scores = [test_results[m] for m in metrics]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, val_scores, width, label='Validation', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, test_scores, width, label='Test', color='seagreen', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Validation vs Test Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.capitalize() for m in metrics])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n{'Metric':<12} | {'Validation':>12} | {'Test':>12}\")\n",
    "print(f\"{'-'*12}-+-{'-'*12}-+-{'-'*12}\")\n",
    "for metric in metrics:\n",
    "    print(f\"{metric.capitalize():<12} | {val_results[metric]:>12.4f} | {test_results[metric]:>12.4f}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(config['paths']['models_dir'])\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = models_dir / 'bayes_classifier.pkl'\n",
    "classifier.save(model_path)\n",
    "\n",
    "print(f\"\\nModel saved successfully!\")\n",
    "print(f\"Path: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.bayes import BayesFeatureExtractor\n",
    "\n",
    "feature_extractor = BayesFeatureExtractor(config)\n",
    "\n",
    "sample_size_per_class = 100\n",
    "sampled_features = []\n",
    "sampled_labels = []\n",
    "\n",
    "print(\"Extracting optimized features from sample data...\")\n",
    "for class_idx, class_name in enumerate(config[\"data\"][\"classes\"]):\n",
    "    count = 0\n",
    "    for item in train_dataset:\n",
    "        label = item[\"label\"]\n",
    "        if label == class_idx and count < sample_size_per_class:\n",
    "            image = item[\"image\"].resize((config[\"data\"][\"image_size\"], config[\"data\"][\"image_size\"]))\n",
    "            features = feature_extractor.extract_features(image)\n",
    "            sampled_features.append(features)\n",
    "            sampled_labels.append(label)\n",
    "            count += 1\n",
    "        if count >= sample_size_per_class:\n",
    "            break\n",
    "    print(f\"  {class_name}: {count} samples\")\n",
    "\n",
    "sampled_features = np.array(sampled_features)\n",
    "sampled_labels = np.array(sampled_labels)\n",
    "\n",
    "print(f\"\\nExtracted feature shape: {sampled_features.shape}\")\n",
    "\n",
    "feature_groups = {\n",
    "    \"Color (HSV)\": list(range(6)),\n",
    "    \"Specular (Metal) - Optimized\": [29, 30],\n",
    "    \"Glass - Optimized\": [32, 33, 34],\n",
    "    \"Trash - Optimized\": [35]\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_idx, (group_name, feature_indices) in enumerate(feature_groups.items()):\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    for class_idx, class_name in enumerate(config[\"data\"][\"classes\"]):\n",
    "        class_mask = sampled_labels == class_idx\n",
    "        class_features = sampled_features[class_mask][:, feature_indices]\n",
    "        mean_values = np.mean(class_features, axis=1)\n",
    "        ax.hist(mean_values, bins=30, alpha=0.5, label=class_name, density=True)\n",
    "    \n",
    "    ax.set_title(f\"{group_name} Distribution\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Mean Feature Value\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend(loc=\"best\", fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Optimized Feature Distributions Across Waste Classes\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Per-Class Feature Statistics (Mean +/- Std) - OPTIMIZED FEATURES\")\n",
    "print(f\"{'='*70}\")\n",
    "for feat_name, feat_indices in feature_groups.items():\n",
    "    print(f\"\\n{feat_name}:\")\n",
    "    for class_idx, class_name in enumerate(config[\"data\"][\"classes\"]):\n",
    "        class_mask = sampled_labels == class_idx\n",
    "        class_features = sampled_features[class_mask][:, feat_indices]\n",
    "        mean_val = np.mean(class_features)\n",
    "        std_val = np.std(class_features)\n",
    "        print(f\"  {class_name:<12}: {mean_val:>8.4f} +/- {std_val:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
