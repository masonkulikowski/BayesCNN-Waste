{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import load_config\n",
    "from src.load_data import load_data, get_class_dist\n",
    "from src.splits import save_splits, load_splits_meta\n",
    "from src.transforms import get_transforms, get_base_transforms, compute_dataset_stats, denormalize, tensor_to_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Total Images per Split:\n",
      "==================================================\n",
      "Train: 3537 images (69.98%)\n",
      "Val:    758 images (15.00%)\n",
      "Test:   759 images (15.02%)\n",
      "Total: 5054 images\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "config = load_config()\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = load_data(split_data=True)\n",
    "total = len(train_dataset) + len(val_dataset) + len(test_dataset)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Total Images per Split:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Train: {len(train_dataset):4d} images ({len(train_dataset) / total * 100:.2f}%)\")\n",
    "print(f\"Val:   {len(val_dataset):4d} images ({len(val_dataset) / total * 100:.2f}%)\")\n",
    "print(f\"Test:  {len(test_dataset):4d} images ({len(test_dataset) / total * 100:.2f}%)\")\n",
    "print(f\"Total: {total:4d} images\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Class Balance Verification\n",
      "================================================================================\n",
      "Class        |           Train |             Val |            Test\n",
      "-------------+-----------------+-----------------+----------------\n",
      "cardboard    |  580 ( 16.4%) |  110 ( 14.5%) |  116 ( 15.3%)\n",
      "glass        |  705 ( 19.9%) |  143 ( 18.9%) |  154 ( 20.3%)\n",
      "metal        |  585 ( 16.5%) |  116 ( 15.3%) |  119 ( 15.7%)\n",
      "paper        |  821 ( 23.2%) |  187 ( 24.7%) |  180 ( 23.7%)\n",
      "plastic      |  647 ( 18.3%) |  166 ( 21.9%) |  151 ( 19.9%)\n",
      "trash        |  199 (  5.6%) |   36 (  4.7%) |   39 (  5.1%)\n",
      "-------------+-----------------+-----------------+----------------\n",
      "Total        | 3537 (100.0%) |  758 (100.0%) |  759 (100.0%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "train_dist = get_class_dist(train_dataset)\n",
    "val_dist = get_class_dist(val_dataset)\n",
    "test_dist = get_class_dist(test_dataset)\n",
    "\n",
    "class_names = list(train_dist.keys())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Class Balance Verification\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Class':<12} | {'Train':>15} | {'Val':>15} | {'Test':>15}\")\n",
    "print(f\"{'-'*12}-+-{'-'*15}-+-{'-'*15}-+-{'-'*15}\")\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_count = train_dist[class_name]\n",
    "    val_count = val_dist[class_name]\n",
    "    test_count = test_dist[class_name]\n",
    "    \n",
    "    train_pct = (train_count / len(train_dataset)) * 100\n",
    "    val_pct = (val_count / len(val_dataset)) * 100\n",
    "    test_pct = (test_count / len(test_dataset)) * 100\n",
    "    \n",
    "    print(f\"{class_name:<12} | {train_count:4d} ({train_pct:5.1f}%) | {val_count:4d} ({val_pct:5.1f}%) | {test_count:4d} ({test_pct:5.1f}%)\")\n",
    "\n",
    "print(f\"{'-'*12}-+-{'-'*15}-+-{'-'*15}-+-{'-'*15}\")\n",
    "print(f\"{'Total':<12} | {len(train_dataset):4d} (100.0%) | {len(val_dataset):4d} (100.0%) | {len(test_dataset):4d} (100.0%)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "splits = [('Train', train_dist, len(train_dataset)), \n",
    "          ('Validation', val_dist, len(val_dataset)), \n",
    "          ('Test', test_dist, len(test_dataset))]\n",
    "\n",
    "for ax, (split_name, dist, total) in zip(axes, splits):\n",
    "    classes = list(dist.keys())\n",
    "    counts = list(dist.values())\n",
    "    \n",
    "    bars = ax.bar(classes, counts, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Waste Category', fontsize=11)\n",
    "    ax.set_ylabel('Number of Images', fontsize=11)\n",
    "    ax.set_title(f'{split_name} Split (n={total})', fontsize=13, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, (cls, count) in enumerate(zip(classes, counts)):\n",
    "        ax.text(i, count + 10, str(count), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TrashNet-specific normalization statistics...\n",
      "\n",
      "Computing dataset statistics...\n",
      "Processing 1000 images...\n",
      "  Processed 500/1000 images\n",
      "  Processed 1000/1000 images\n",
      "\n",
      "Dataset Statistics:\n",
      "  Mean: [0.6768453121185303, 0.6304212808609009, 0.5802809000015259]\n",
      "  Std:  [0.18257321417331696, 0.18208937346935272, 0.1952831894159317]\n",
      "\n",
      "======================================================================\n",
      "Normalization Statistics Comparison\n",
      "======================================================================\n",
      "\n",
      "Dataset         |     R Mean |     G Mean |     B Mean\n",
      "----------------+------------+------------+-----------\n",
      "ImageNet        |     0.4850 |     0.4560 |     0.4060\n",
      "TrashNet        |     0.6768 |     0.6304 |     0.5803\n",
      "\n",
      "Dataset         |      R Std |      G Std |      B Std\n",
      "----------------+------------+------------+-----------\n",
      "ImageNet        |     0.2290 |     0.2240 |     0.2250\n",
      "TrashNet        |     0.1826 |     0.1821 |     0.1953\n",
      "======================================================================\n",
      "\n",
      "Note: Using ImageNet stats is recommended for transfer learning with pretrained models.\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing TrashNet-specific normalization statistics...\\n\")\n",
    "full_dataset = load_data(split_data=False)\n",
    "\n",
    "trashnet_stats = compute_dataset_stats(full_dataset, config, num_samples=1000)\n",
    "\n",
    "imagenet_stats = {\n",
    "    'mean': config['normalization']['mean'],\n",
    "    'std': config['normalization']['std']\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Normalization Statistics Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Dataset':<15} | {'R Mean':>10} | {'G Mean':>10} | {'B Mean':>10}\")\n",
    "print(f\"{'-'*15}-+-{'-'*10}-+-{'-'*10}-+-{'-'*10}\")\n",
    "print(f\"{'ImageNet':<15} | {imagenet_stats['mean'][0]:>10.4f} | {imagenet_stats['mean'][1]:>10.4f} | {imagenet_stats['mean'][2]:>10.4f}\")\n",
    "print(f\"{'TrashNet':<15} | {trashnet_stats['mean'][0]:>10.4f} | {trashnet_stats['mean'][1]:>10.4f} | {trashnet_stats['mean'][2]:>10.4f}\")\n",
    "\n",
    "print(f\"\\n{'Dataset':<15} | {'R Std':>10} | {'G Std':>10} | {'B Std':>10}\")\n",
    "print(f\"{'-'*15}-+-{'-'*10}-+-{'-'*10}-+-{'-'*10}\")\n",
    "print(f\"{'ImageNet':<15} | {imagenet_stats['std'][0]:>10.4f} | {imagenet_stats['std'][1]:>10.4f} | {imagenet_stats['std'][2]:>10.4f}\")\n",
    "print(f\"{'TrashNet':<15} | {trashnet_stats['std'][0]:>10.4f} | {trashnet_stats['std'][1]:>10.4f} | {trashnet_stats['std'][2]:>10.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nNote: Using ImageNet stats is recommended for transfer learning with pretrained models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Normalization Statistics\n",
    "\n",
    "Compare ImageNet normalization statistics with TrashNet-specific statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample images from each class:\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(len(class_names), 3, figsize=(12, len(class_names) * 3))\n",
    "\n",
    "base_transform = get_base_transforms(config)\n",
    "val_transform = get_transforms(config, split='val')\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    label_names = train_dataset.features['label'].names\n",
    "    for i in range(len(train_dataset)):\n",
    "        item = train_dataset[i]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "        if label_names[label] == class_name:\n",
    "            axes[idx, 0].imshow(image)\n",
    "            axes[idx, 0].set_title(f'{class_name} - Original')\n",
    "            axes[idx, 0].axis('off')\n",
    "            \n",
    "            resized = base_transform(image)\n",
    "            axes[idx, 1].imshow(resized.permute(1, 2, 0))\n",
    "            axes[idx, 1].set_title(f'Resized to {config[\"data\"][\"image_size\"]}x{config[\"data\"][\"image_size\"]}')\n",
    "            axes[idx, 1].axis('off')\n",
    "            \n",
    "            preprocessed = val_transform(image)\n",
    "            denorm = denormalize(preprocessed, config)\n",
    "            axes[idx, 2].imshow(denorm.permute(1, 2, 0))\n",
    "            axes[idx, 2].set_title('Normalized (ImageNet stats)')\n",
    "            axes[idx, 2].axis('off')\n",
    "            \n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Splits saved to data/splits\n",
      "\n",
      "Files created:\n",
      "  - train_indices.txt (3537 samples)\n",
      "  - val_indices.txt (758 samples)\n",
      "  - test_indices.txt (759 samples)\n",
      "  - splits.json (complete metadata)\n"
     ]
    }
   ],
   "source": [
    "splits_info = save_splits(train_dataset, val_dataset, test_dataset, config)\n",
    "\n",
    "print(f\"\\n✓ Splits saved to {config['paths']['splits_dir']}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - train_indices.txt ({splits_info['train_size']} samples)\")\n",
    "print(f\"  - val_indices.txt ({splits_info['val_size']} samples)\")\n",
    "print(f\"  - test_indices.txt ({splits_info['test_size']} samples)\")\n",
    "print(f\"  - splits.json (complete metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL SPLIT SUMMARY\n",
      "============================================================\n",
      "\n",
      "Dataset: garythung/trashnet\n",
      "Random Seed: 42\n",
      "\n",
      "Split Distribution:\n",
      "  Training:   3537 images (70.0%)\n",
      "  Validation:  758 images (15.0%)\n",
      "  Test:        759 images (15.0%)\n",
      "  Total:      5054 images\n",
      "\n",
      "Number of Classes: 6\n",
      "Classes: cardboard, glass, metal, paper, plastic, trash\n",
      "\n",
      "Split files saved to: data/splits\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "total_images = splits_info['total']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL SPLIT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nDataset: {splits_info['dataset_name']}\")\n",
    "print(f\"Random Seed: {splits_info['seed']}\")\n",
    "print(f\"\\nSplit Distribution:\")\n",
    "print(f\"  Training:   {splits_info['train_size']:4d} images ({splits_info['train_size']/total_images*100:.1f}%)\")\n",
    "print(f\"  Validation: {splits_info['val_size']:4d} images ({splits_info['val_size']/total_images*100:.1f}%)\")\n",
    "print(f\"  Test:       {splits_info['test_size']:4d} images ({splits_info['test_size']/total_images*100:.1f}%)\")\n",
    "print(f\"  Total:      {total_images:4d} images\")\n",
    "print(f\"\\nNumber of Classes: {config['data']['num_classes']}\")\n",
    "print(f\"Classes: {', '.join(config['data']['classes'])}\")\n",
    "print(f\"\\nSplit files saved to: {config['paths']['splits_dir']}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
